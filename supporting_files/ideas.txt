/usr/local/mysql/bin/mysql -u root -pGobronxbombers2

to fix memory leak issues: rm -rf /opt/homebrew/var/mysql
rm -rf /opt/homebrew/bin/mysql

/usr/local/mysql-8.0.26-macos11-arm64/support-files/mysql.server start

/opt/homebrew/bin/mysql.server

ps aux | grep mysqld

/opt/homebrew/var/mysql/localhost.log
    - see localhost.err



https://stackoverflow.com/questions/58466272/homebrew-mysql-8-0-18-on-macos-10-15-catalina-wont-run-as-service



kill -9 id

TPC-H
    - https://www.metricfire.com/blog/a-modern-guide-to-mysql-performance-monitoring/
    - https://dev.mysql.com/doc/refman/8.0/en/server-status-variables.html
    - https://stackoverflow.com/questions/1733507/how-to-get-size-of-mysql-database
    - https://docs.verdictdb.org/tutorial/tpch/
    - https://www.tpc.org/tpc_documents_current_versions/current_specifications5.asp
    - https://dev.mysql.com/doc/heatwave/en/mys-hw-tpch-quickstart-create-database-import.html

    - https://github.com/Percona-Lab/sysbench-tpcc
    - https://github.com/akopytov/sysbench

    - https://github.com/electrum/tpch-dbgen
    - https://github.com/dhuny/tpch/tree/main VERY USEFUL
    - https://github.com/pola-rs/tpch/tree/700a440212aada40fbe3431591cb19d0cb0b530e VERY USEFUL

    - /Users/jamespetullo/Downloads/TPC_H_official/dbgen

    Need to create one additional table:
        create table if not exists revenue0 (supplier_no int, total_revenue int);
        insert into revenue0
            select l_suppkey, sum(l_extendedprice * (1 - l_discount)) from lineitem where l_shipdate >= date '1993-01-01' and l_shipdate < date '1993-01-01' + interval '3' month group by l_suppkey;


Indices:
    - https://stackoverflow.com/questions/14143813/find-out-usage-statistics-of-mysql-indices
    - https://dba.stackexchange.com/questions/20038/how-can-i-tell-if-an-index-is-being-used-to-sort-in-mysql
    - https://stackoverflow.com/questions/31372353/how-to-check-the-query-is-using-the-indexes-in-mysql
    - https://dev.mysql.com/doc/refman/8.0/en/explain-output.html

Knob params: https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html

Loss and gradients:
    - https://stackoverflow.com/questions/73840143/in-pytorch-how-do-i-update-a-neural-network-via-the-average-gradient-from-a-lis
    - https://discuss.pytorch.org/t/how-to-get-gradient-of-loss/16955
    - https://stackoverflow.com/questions/57248777/backward-function-in-pytorch
    - https://stackoverflow.com/questions/65947284/loss-with-custom-backward-function-in-pytorch-exploding-loss-in-simple-mse-exa
    - https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html
    - https://stackoverflow.com/questions/57580202/whats-the-purpose-of-torch-autograd-variable
    

Normalization:
    - https://discuss.pytorch.org/t/best-way-to-normalize-the-input/139557


Output layer activations:
    - https://stackoverflow.com/questions/56415450/how-to-set-up-a-neural-network-so-that-it-have-in-output-only-0-or-1
    - https://datascience.stackexchange.com/questions/92748/how-to-make-a-neural-network-output-a-specific-number-out-of-a-certain-range
    - sigmoid: binary
    - softmax: multiclass
    - both between 0 and 1, however

    - try Huberloss for DQN: makes loss less sensitive to outlyers


Notes:
    - created folder "lib" in /usr/local/
    - sudo ln -s /usr/local/mysql/lib/libssl.1.1.dylib /usr/local/lib/libssl.1.1.dylib\n
    - sudo ln -s /usr/local/mysql/lib/libcrypto.1.1.dylib /usr/local/lib/libcrypto.1.1.dylib\n

    - throughput and latency is calculated on the tables in tpcc100
    - need a table tpch for indices work

    - ./tpcc_start -h 127.0.0.1 -d tpcc100 -uroot -p "Gobronxbombers2" -w 10 -c 6 -r 10 -l 30 -i 2 > /Users/jamespetullo/atlastune/tpc/tpcc/performance_outputs/run_test2.txt

    - current database: tpch1 (1GB)
        - see folder "TPC_H_OFFICIAL"

    - Starting indices option: https://github.com/dimitri/tpch-citus/blob/master/schema/tpch-index.sql

    - Idea: using QPH or raw latency from running the queries might take too long in a standard training cycle, however, query cost can be used, but every n cycles, the QPH can be found, and if QPH(i+1) < QPH(i), that is, a drop in overall raw performance, subsequent penalties can be applied to the rewards stored for transitions in the minibatch indexed under the last n cycles. That way, when transitions are sampled, the updated rewards can be used

    - Idea: every n cycles, run the standard latency and throughput tests, and go back into memory buffer and make penalizations if latency and throughput have degraded (i.e too many indices have been created)

    - There might be enough of a positive gradient in the experience replay if most rewards are negative

    - Idea: clip rewards to be between range of -10 and 10
    - Idea: alter tables to remove primary keys
    - Look into using tracxn database
    - Perhaps try TPC-DS?
    - TPC-E?
    - https://github.com/TPC-Council/HammerDB/
    - https://github.com/cmu-db/benchbase
    - TPC-H works?
    - with TPC-H, perhaps clearing out the dummy text blobs did the trick
    - Not every query is equal: more time-intensive queries should have heigher priority than inherently shorter/faster queries, perhaps need weighting 
    - Note: might need to initialize experience replay with self.actor instead of random process?
    - https://www.cockroachlabs.com/docs/stable/performance-benchmarking-with-tpcc-large
    - https://www.cockroachlabs.com/docs/stable/performance
    - https://medium.com/@chachia.mohamed/stress-testing-in-centos-using-sysbench-and-stress-commands-2c4530122c45
    - https://github.com/Percona-Lab/sysbench-tpcc
    - https://dbmsbenchmarker.readthedocs.io/en/latest/
    - https://www.howtoforge.com/how-to-benchmark-your-system-cpu-file-io-mysql-with-sysbench
    - https://severalnines.com/blog/how-benchmark-performance-mysql-mariadb-using-sysbench/
    - https://blog.purestorage.com/purely-informational/how-to-benchmark-mysql-performance/
    - https://www.percona.com/blog/sysbench-with-support-of-multi-tables-workload/
    - https://www.bigdatalyn.com/2022/03/03/Mysql_sysbench_Tips/

Total rewards:
    R = [w1*(cost(wkld, 0) - cost(wkld, i))/cost(wkld, 0), 
        w2*(latency(wkld, 0) - latency(wkld, i))/latency(wkld, 0),
        w3*(throughput(wkld, i) - throughput(wkld, 0))/throughput(wkld, 0)]

    r = min(R),
        math.avg(R),
        sum(R)

    w1, w2, w3 = 1, 1, 1
    
    Evolving weights:
        1. apply with w1, w2, w3
        2. over n iterations, observe the accumulated rewards r1, r2, rn
        3. at the end of the iteration, store ((w1, w2, w3), avg(r1, r2, ... rn))
        4. select two tuples using fitness proportional sampling:
            (w1, w2, w3) and (w1_1, w2_1, w3_3)
            perform crossover and add random noise to a weight. Go back to 1 


To test knob tuner:
    - use sysbench tpcc with scale = 75
    - also try basic sysbench for more clear throughput metric
    - run ./tpcc_load on 1000, drop all indices, etc.
        - see if ./tpcc_start has more reliable metrics that run faster

Putting it all together:
    - write custom updates and inserts for tpcc100
        - use that to calculate latency and throughput

    - based on results from index selection, can probably increase scale of sysbench_tpcc to 50


#TODO: anneal ddpg noise_scale

Might need to increase size of min_memory in knobs to be the default on my system

Todo: need delay between apply_knob_config and tpcc_metrics?

TODO: for training only knob tuning, remove indices from the state 

https://ottertune.com/blog/mysql-configuration-settings-list


A faster index selector:
    - use query planner/query analysis to get initial set of columns that could use an index
        - Or, index every column, use query-planner to identify indices that are used
    - if the suggested q-value points to a column that is not in the set, penalize
        - else, apply knob configuration, get the delta cost change, and continue


Sysbench:

    - https://blog.csdn.net/cxin917/article/details/81557453
    - https://imysql.com/wp-content/uploads/2014/10/sysbench-manual.pdf
    - https://www.mortensi.com/2021/06/how-to-install-and-use-sysbench/
    - https://webhostinggeeks.com/howto/how-to-use-sysbench-to-test-database-performance-on-a-linux-machine/
    - https://github.com/HustAIsGroup/CDBTune/blob/abfbc25b6bc223c8be158b453fdf76bef43030eb/scripts/run_sysbench.sh#L13


Dockerizing:
    - https://mothishdeenadayalan.medium.com/containerizing-a-python-app-mysql-python-docker-1ce64e444ed9
    - https://stackoverflow.com/questions/9766014/connect-to-mysql-on-amazon-ec2-from-a-remote-server


Connecting to AWS:
    - https://www.alibabacloud.com/help/en/rds/apsaradb-rds-for-mysql/change-the-size-of-the-innodb-buffer-pool-for-an-apsaradb-rds-for-mysql-instance
    - https://repost.aws/knowledge-center/low-freeable-memory-rds-mysql-mariadb
    - https://zenliu.medium.com/sysbench-1-1-installation-for-aws-ec2-instance-running-amazon-linux-a330b1cce7a7
    - https://medium.com/@amanlonare/performance-testing-of-rds-mysql-database-using-sysbench-d95eca450fa7
    - https://medium.com/@sidshnkar/different-ways-to-ssh-into-your-aws-ec2-instance-2f34b2c3cba9
    - https://stackoverflow.com/questions/76461475/trying-to-install-mysql-server-on-aws-ec2-amazon-linux-but-getting-problem-con

    Simply host MySQL on AWS EC2:
        - https://docs.aws.amazon.com/whitepapers/latest/optimizing-mysql-on-ec2-using-amazon-ebs/mysql-on-aws-deployment-options.html
        - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_GettingStarted.CreatingConnecting.MySQL.html
        - https://medium.com/@waya.ai/quick-start-pyt-rch-on-an-aws-ec2-gpu-enabled-compute-instance-5eed12fbd168
        - https://www.geeksforgeeks.org/how-to-install-mysql-on-aws-ec2/
        - https://dev.to/rikinptl/setting-up-mysql-on-amazon-ec2-using-homebrew-3nmc

Next Steps:
    - Email Chris about Dr. Olga's compute cluster
    - Look into AWS vs Google Cloud
    - Investigate TPC-H workload
    - Test index tuner on sysbench
    - refactor MARL integration code


So far:
    best Knob tuner and Index Selector in models_v3.py

    Knob tuning with memory reply size truncated seems to work the best

    Probably need to set workload_exec_time to 60 seconds on cloud tuning for greater stability

    Store model snapshots in DDPG and retrieve snapshot from highest performer


    Convergence:
        explore until noise_eliminate, then run for 50 iterations. Stop after that, saving model checkpoints.
            chose the best model from the checkpoint options.

    Ask people who know more about PyTorch for feedback

        - Am I using eval/train correctly?

    Use CDBTune and SmartIx models, build wrapper around their training process

    https://stackoverflow.com/questions/11091414/how-to-stop-mysqld

Stopping MySQL ID:

brew services stop mysql
sudo mysqld stop
sudo killall mysqld



Current approach:
    - run existing methodology on AWS with larger workloads over a longer period of time 
        - execute benchmarks for longer
        - stress test metrics for longer


If that fails:
    - look into porting CDBtune's ddpg
    - BO-based methods
    - train on more knobs (6+)

https://teaching-on-testbeds.github.io/hello-chameleon/
https://code.visualstudio.com/docs/remote/ssh
https://dev.to/khairunnaharnowrin/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent-4c6b


Install github on instance, pull project repo, etc etc.

Chameleon Cloud SSH:
    name: Knob Tuning
    Knob Tuning.pem
    Public Key: 
    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCjYYXqPHCQNWbUCTTQ4kdnmSN7qQ2lOb50Z4wr9fuEdhoA+FH9vtIwaM75dAYi+/LBcu7e19F36/OrX5r5z7Q+xE1F57C3R+mZfPBDpSUB9CtTOu/apMT/ThHsV6fhKEACz6u5bfBTvCxOIv2xq9ameUBH63+wIHJyNGefvC693O5mHcFOo9nHb1ZEt1Z6vMZVyABIKsEfLsbfJv05/aua+lYxYj7Nj0VyAl1ucrwTfFdPyA0XZANGQw6LxLhZqILKVxGKFxKn6/qfjMKOfYpsxecQwYjdv3ojMQIghInQQhtQO8L9TU3aWuJOXjickeh3OlgHibTqAJT2o3cxeHLF Generated-by-Nova
    chmod 400 /Users/jamespetullo/Downloads/Knob\ Tuning.pem
    ssh -i ~/.ssh/id_rsa_chameleon cc@129.114.109.248

    ssh -l jamespetullo@brandeis.edu -i .ssh/yourkey.pem public-ec2-host

    ssh-add ~/.ssh/id_rsa_chameleon

    ssh-keygen -t rsa -f ~/.ssh/id_rsa_chameleon
    passphrase: zorro16


    Your identification has been saved in /Users/jamespetullo/.ssh/id_rsa_chameleon
    Your public key has been saved in /Users/jamespetullo/.ssh/id_rsa_chameleon.pub
    The key fingerprint is:
    SHA256:zZCxL3UXyuBC+5Qzf79ALwTUuvKINAvp1cqnl+ziPfw jamespetullo@localhost
    The key's randomart image is:
    +---[RSA 3072]----+
    |        o ... .  |
    |       . *.+ o . |
    |        * B.= .  |
    |         X *..   |
    |      . S = oo.  |
    |     o + + .o... |
    |    . = B =  o ..|
    |     . *.O .  o .|
    |      .o*ooE   . |
    +----[SHA256]-----+


    USE THIS: ssh -i ~/.ssh/id_rsa_chameleon cc@129.114.109.248


    ssh-keygen -t -C "jpetullo14@gmail.com"

    git remote set-url origin git@github.com:Ajax12345/atlastune.git

    UPDATE user SET plugin='mysql_native_password' WHERE User='root';
    FLUSH PRIVILEGES;
    ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'Gobronxbombers2';

    sudo chmod -R 777 /etc/mysql


    To run detached process:
    export ATLASTUNE_ENVIRONMENT=CC
    sudo -E nohup python3 models_v2.py &

    to monitor status: ps -ef | grep python