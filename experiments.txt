
04/04/2024 @ 6:28 PM
    - Currently plan on trying TPC-C with all primary indices dropped.
    - If this does not work on standard model, will switch to TPC-H
    - Might want to run installation of TPC-H again, we cleared out the large text fields for the sake of memory. It might not matter, though.


04/08/2024 @ 12:20 PM
    - 500 iterations
    - idea: change actor/critic network architecture
        - add more layers
        - add fewer?
    - try with larger batch size
    - see that only a small number of -20 rewards occur
    - there is a poor solving gradient: assuming you converge on index i, which is optimal, then anything better will simply need to a negative result, since it has plateued and cannot find a better solution
        - trick might be to check performance of new candidate relative to any of the existing solutions in the memory buffer
        - should be measured against the best seen so far, at any point
        - new reward:
            - number of candidates it is better than?
            - number of better solutions, number of worse solutions?
            - in terms of total cost

    - TODO: reward is -1*query_cost
        - might need to normalize query_cost
    - Also try the reward as being -1*execution time    
    - Try other reward formulas
    - detect convergence: that is, when suggested solutions are not noticeable better than the others seen, after a certain period of time
    - if -1*query_cost or -1*execution_time does not work, then get baseline costs for workload and then find max(original - new for original in baselines)
    - check that weight updating is correct
    - try on TPC-H
    - repurpose old reward function based on percentage change, but the w1 is the original (first) non-indexed state

04/09/2024 @ 11:29 AM
    reward function is now -1*total_query_cost
    experience replay: custom_exprience_replay2024-04-0911:36:45523674.json
    replay batch size: 50
    weight updates: target * t + old* (1 - t)
    reward function: compute_total_cost_reward

    - Observations:
        - total cost too large, had to normalize rewards, but loss did not decrease

    - TODO:
        - change weight updates to be: target * (1 - t) + old * t
        - have reward be the difference between default configuration and proposed configuration
            - clip at -10 and 10


04/09/2024 @ 11:58 AM
    reward now  difference between default configuration and proposed configuration
    reward function: compute_cost_delta
    weight updates: target * t + old* (1 - t)
    
    Oberservations:
        - loss does not decrease
        - 99% of the rewards were = 5


04/09/2024 @ 1:22 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json

    ideas:
        plot size of reward given out at each iteration
        change around critic and actor architecture
        pass to critic the activated action values
        reward function does not just approximate from experience_replay[0], but a random sample of prior solutions
        using incorrect loss function?
        - try on TPC-H

    - TODO: plot returned reward at each iteration
        - initialize weights differently (see papers)
            https://www.reddit.com/r/reinforcementlearning/comments/ookni2/sac_critic_loss_increases/
        - decrease the size of the critic network

        - https://stackoverflow.com/questions/58004237/critic-loss-for-rl-agent

04/09/2024 @ 2:00 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json

    plotting rewards per iteration

    using _init_weights

    observations:
        reward per iteration increasing!!!

    TODO:
        - remove _init_weights
        - try on 500 iterations
        - try on TPC-H
        - update reward function to sample from elsewere in the experience replay
        - decrease the size of the critic network
        - train over 5 epochs, find the mean of each reward at every iteration
        - compare with purely random search

04/09/2024 @ 3:57 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json

    300 iterations over five epochs


    Observation:    
        - logarithmic increase

     TODO: 
        - run purely random over five epochs

    - Try DQN?
    - Implement this algo more closely:
        - https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode
        - differentiate between Q(s, a) and Q(s, u(s))

04/09/2024 @ 6:09 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json
    
    - using both Q(s, a) and Q(s, u(s))
    - target_param.data * self.config['tau'] + param.data * (1-self.config['tau'])
    - epsilon = 0.75
    - batch_size = 20

    Ideas
        - have bound = 2 in generate_random_index
        - change network layers
        - possible overfitting? try again with the tuner context manager inside the loop
        - increase batch size


04/09/2024 @ 7:49 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json
    true epoch tuning

    Observations: consistent growth

    Output: outputs/rl_ddpg.json

    Ideas:
        - normalize reward
    

04/09/2024 @ 8:36 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json
    
    100 iterations

    Output: outputs/rl_ddpg1.json

    - normalization of batch rewards
        - Observation: no major difference between reward normalization and no normalization
        - a little more stable, but not by much

    Ideas:
        - reward could be original_total_cost - new_total_cost, normalized across batch
        - reduce network size
        - new reward functions
        - try on TPC-H
        - reward function could be based on query execution time
        - try DQN

04/09/2024 @ 9:48 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json
    
    Output: outputs/rl_ddpg2.json

    Same as last experiement, but over 300 iterations

    Cannot break past the reward threshold of six


04/10/2024 @ 10:46 AM
    -1*predicted_q_value

    Output: outputs/rl_ddpg3.json
    more performant than previously: breaks threshold of six

    TODO: 
        - remove epsilon greedy
        - simplify network models

04/10/2024 @ 11:39 AM
    -1*predicted_q_value

    removed epsilon greedy

    Output: outputs/rl_ddpg4.json

    Epsilon greedy fails: 
        - plateaus at 6.3

    TODO: 
        - add noise to actor in place of epsilon greedy
        - simplify network models

        put weight updates in body of with torch.no_grad():



04/10/2024 @ 11:59 AM
    -1*predicted_q_value

    Output: outputs/rl_ddpg5.json
    
    put weight updates in body of with torch.no_grad():

    Oberservations:
        - poorer performance

    TODO: 
        - add noise to actor in place of epsilon greedy
            - np.random.randn(5)
        - simplify network models


04/10/2024 @ 12:29 PM
    -1*predicted_q_value

    Output: outputs/rl_ddpg6.json
            outputs/rl_ddpg7.json
            outputs/rl_ddpg8.json
            outputs/rl_ddpg9.json

    
    Adding actor noise in place of epsilon greedy
        noise_scale: 0.25

    TODO: 
        - simplify network models


04/11/2024 @ 9:30 AM
    -1*predicted_q_value

    Output: outputs/rl_ddpg10.json
            
    Trying to decrease volatility

    noise_scale: 0.05
        - no meaningful volatility decrease

    TODO: 
        - simplify network models
        - update rewards so that change of 0 is not included in the average

04/11/2024 @ 10:00 AM

    Output: outputs/rl_ddpg11.json
            outputs/rl_ddpg12.json

    Simplified network architecture
        
    Trying to decrease volatility

    noise_scale: 0.05

    Oberservations:
        - approaches 8
        - BEST DDPG TO DATE
        - needs to be run over more iterations to confirm, though

    TODO: 
        - update rewards so that change of 0 is not included in the average
        - could also subtract -4 from each reward to add more of a penalty and encourage further exploration
        - may need to run on larger databases for a better problem gradient
        - another possible reward function:
            r = (cost(workload, i) - cost(workload, i+1))/cost(workload, 0)
        
        - penalize if agent chooses an index that is not used in any expression?
        - in DQN, copy weights after long time lapses

04/11/2024 @ 7:49 PM

    Output: outputs/rl_ddpg14.json

    Normalizing index configuration and metrics

    Observations:
        - no meaningful change


04/12/2024 @ 11:57 AM
    Using DQN

    Experience replay:
        experience_replay/dqn_index_tune/experience_replay_tpcc100_2024-04-1212:00:36529360.json


    'weight_copy_interval':20
    'epsilon':0.7,
    'epsilon_decay':0.01,
    'batch_sample_size':30


    Output: outputs/rl_dqn1.json

    Observations:
        clearly steady learning!!

    TODO: 
        - update rewards so that change of 0 is not included in the average
        - could also subtract -4 from each reward to add more of a penalty and encourage further exploration
        - may need to run on larger databases for a better problem gradient
        - another possible reward function:
            r = (cost(workload, i) - cost(workload, i+1))/cost(workload, 0)
        
        - penalize if agent chooses an index that is not used in any expression?
        - in DQN, copy weights after long time lapses
        - try Huber Loss
        - train over multiple different epochs
        - need more exploration:
            - annealing should either be removed or cooling factor significantly decreased
        
        - decrease learning rate


04/12/2024 @ 2:19 PM
    Using DQN

    Four training runs for mean calculation
    
    Experience replay:
        experience_replay/dqn_index_tune/experience_replay_tpcc100_2024-04-1212:00:36529360.json

    Output: 
        outputs/rl_dqn2.json
        outputs/rl_dqn3.json
        outputs/rl_dqn4.json

    'lr':0.0001,
    'gamma':0.9,
    'weight_copy_interval':10,
    'tau':0.9999,
    'epsilon':1,
    'epsilon_decay':0.001,
    'batch_sample_size':50

    Observations:
        - need more exploration
            - cooling factor needs to be much smaller


04/12/2024 @ 3:40 PM
    DQN

    hard-coded e = 0.4

    no substantial growth beyond 6


04/12/2024 @ 3:46 PM
    DQN

    Experience replay: experience_replay/dqn_index_tune/experience_replay_tpcc100_2024-04-1212:00:36529360.json
    - {'epsilon':1, 'epsilon_decay':0.004}
    - 500 iterations
    - 4 epochs
    
    Output: rl_dqn6.json

    Observations:
        converges around 5

04/12/2024 @ 4:01 PM
    DQN

    - Generates new experience replay every time
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.001}
    - 500 iterations
    - 4 epochs

    Output: outputs/rl_dqn7.json

    Observations:
        - converges to around 6

    Ideas:
        - Need to experiment with annealing, weight_copy_interval, etc
        - Also need epochs for training that preserve models, optimizer, etc. 


04/12/2024 @ 8:22 PM

    DQN

    - running 5 epochs without resetting models
    - initial memory replay size = 150, reset at every epoch
    - 500 iterations per epoch

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.001}

    Output:
        outputs/tuning_data/rl_dqn2.json


04/13/2024 @ 11:36 AM (experiment 25)
    DQN

    - running 5 individual epochs (model resetting)
    - replay memory size: 150
        - new experience replay created at each epoch
    
    - 500 iterations per epoch
    - includes new delta cost metric
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.006}
    - reward_func = 'compute_total_cost_reward'

    Output: 
        outputs/tuning_data/rl_dqn3_5.json

    Observations:
        - need to reduce epsilon decay further (try 0.003)
    

04/13/2024 @ 12:06 PM (experiment 26)
    DQN

    - Same as experiment 25, except:
        {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.0025}
    
    Output: 
        outputs/tuning_data/rl_dqn3.json

    Ideas:
        - increase batch sample size
        - increase number of iterations before weights are swapped

        Possible reward functions:
            (cost(0, workload) - cost(i+1, workload))/cost(0, workload) + (cost(i, workload) - cost(i+1, workload))/cost(i, workload)

            if cost(0, workload) > cost(i+1, workload):
                if cost(i, workload) > cost(i+1, workload):
                    return 10
                return 5

            return -5


            (cost(workload, i) - cost(workload, i+1))/cost(workload, 0)

            (min(costs[j:k]) - cost(i+1, workload))/min(costs[j:k])


04/13/2024 @ 7:28 PM (experiment 27)
    DQN
    Four epochs, 500 iterations

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.0025}
    
    reward function: compute_min_step_cost

    Output: 
        outputs/tuning_data/rl_dqn4.json




04/13/2024 @ 7:28 PM (experiment 28)
    DQN
    Four epochs, 500 iterations

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.0025}
    
    reward function: compute_total_cost_reward

    Trying total cost reward with a smaller epsilon decay value to see if results from experiment 25 can be replicated

    Output: 
        outputs/tuning_data/rl_dqn5.json


04/13/2024 @ 8:50 PM (experiment 28_1)
    DQN
    Four epochs, 500 iterations
    Same as experiment 28, attempting to replicate results

    Output: 
        outputs/tuning_data/rl_dqn6.json


04/13/2024 @ 8:50 PM (experiment 29)
    DQN
    Four epochs, 500 iterations
    Same as experiment 28, attempting to replicate results

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.007}

    compute_total_cost_reward
    
    Output: 
        outputs/tuning_data/rl_dqn7.json

04/13/2024 @ 9:52 PM (experiment 30)
    DQN
    Four epochs, 500 iterations
    Same as experiment 28, attempting to replicate results

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003}

    compute_increment_step_cost
    
    Output: 
        outputs/tuning_data/rl_dqn8.json

    Some interesting growth. Experiment should be run again to see if results can be replicated

    Currently the best configuration



04/14/2024 @ 10:25 AM (experiment 31)
    DQN

    Q-value action layer now has length of len(indices) + 1
        - corresponding to a "do nothing" action

    reward function: compute_cost_delta_per_query

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003}

    Output:
        outputs/tuning_data/rl_dqn10.json

    Observations:
        no meaningful impact


04/14/2024 @ 10:48 AM (experiment 32)
    DQN

    Running on TPCH1

    reward function: compute_cost_delta_per_query

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003}

    Output:
        outputs/tuning_data/rl_dqn11.json


    A bit more volatile


04/14/2024 @ 12:43 PM (experiment 33)
    DQN

    Running on TPCH1

    reward function: compute_total_cost_reward

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.006}

    Output:
        outputs/tuning_data/rl_dqn12.json

    Ideas:
        - add "no action" option
        - run over 1000 iterations

    Observations:
        - compute_total_cost_reward seemingly effective
        - could be normalized by dividing by the initial total cost
        - https://news.ycombinator.com/item?id=40028111
        - Would not the query cost calculations consider available disk space, etc?

    Questions:
        - weight_copy_interval size?
        - batch size?
        - experience_replay size?
    

04/16/2024 @ 4:00 PM (experiment 34)
    - DQN
    - running on sysbench_tpcc
        - scale = 25

    - 4 epochs, 400 iterations
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.006}

    Output:
        outputs/tuning_data/rl_dqn14.json

    Ideas:
        - can probably scale up sysbench_tpcc to scale = 50
        - instead of initial index configuration as part of the state, vectorize the query payload

04/16/2024 @ 9:00 PM (experiment 35)
    - DQN
    - running on tpcc_1000

    - 4 epochs, 400 iterations
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.006}

    Output:
        outputs/tuning_data/rl_dqn15.json


######################### KNOB TUNING #########################

04/18/2024 @ 4:25 PM (experiment 36)
    - DDPG
    - Might need to increase size of min_memory in knobs to be the default on my system
    - need delay between apply_knob_config and tpcc_metrics?
        - IMPORTANT: verify sequential action here
    - for training only knob tuning, remove indices from the state 
    - should "selected_action" be saved in memory replay without noise added to it?

    reward function: compute_delta_min_reward

    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':0.5, 'noise_decay':0.001, 'batch_size':20, 'tpcc_time':4}
    - iterations: 100
            'alr':0.001,
            'clr':0.001

    Ideas:
        - take average of rewards instead of min
        - pass larger time value to tpcc_metrics
        - #TODO: increase noise_scale for more exploration
        - verify that knobs being used are useful
        - increase database size
        - use sysbench_tpcc
        - use cdbtune's reward function
    
    Knobs to use:
        - innodb_adaptive_flushing_lwm (0, 70)
        - innodb_adaptive_max_sleep_delay *** (0, 1000000)
        - innodb_change_buffering *** ['none', 'inserts', 'deletes', 'changes', 'purges', 'all']
        - innodb_max_dirty_pages_pct [0, 99, 75]
        - innodb_max_dirty_pages_pct_lwm [0, 99, 0]
        - max_heap_table_size [16384, memory_size*5]
        - binlog_cache_size
        - binlog_max_flush_queue_time
        - eq_range_index_dive_limit *** [0, 10000, 200]
        - host_cache_size ** [0, 65536]
        - innodb_adaptive_flushing *
        - innodb_autoextend_increment
        - innodb_buffer_pool_load_at_startup
        - innodb_change_buffer_max_size ***
        - innodb_commit_concurrency **
        - innodb_concurrency_tickets ** [1, 4294967295]
        - innodb_flush_log_at_timeout **
        - innodb_flushing_avg_loops ***
        - innodb_ft_sort_pll_degree [1, 16]
        - innodb_log_buffer_size *** [1048576, 4294967295]
        - innodb_max_purge_lag *
        - innodb_old_blocks_pct **
        - innodb_old_blocks_time **
        - innodb_rollback_segments *
        - innodb_ddl_buffer_size *** [65536, 4294967295]
        - join_buffer_size ***
        - max_binlog_cache_size *?
        - max_binlog_size
        - max_seeks_for_key **
        - max_write_lock_count *
        - innodb_lock_wait_timeout [1, 	1073741824]
        - query_alloc_block_size **

    Important knobs: query_cache_size, innodb_buffer_pool_size, join_buffer_size, eq_range_index_dive_limit


    Experience replay:
        experience_replay/ddpg_knob_tune/tpcc_1000_2024-04-1815:57:59283850.json

    Output:
        outputs/knob_tuning_data/rl_ddpg4.json


04/18/2024 @ 4:43 PM (experiment 37)
    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':0.8, 'noise_decay':0.004, 'batch_size':20, 'tpcc_time':4}
    - iterations: 500
            'alr':0.001,
            'clr':0.001

    reward function: compute_delta_min_reward

    Experience replay:
        experience_replay/ddpg_knob_tune/tpcc_1000_2024-04-1815:57:59283850.json

    Output:
        outputs/knob_tuning_data/rl_ddpg5.json

    Observations: might be overshooting: try smaller learning rate

04/18/2024 @ 4:43 (experiment 38)
    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}
    - iterations: 500
            'alr':0.001,
            'clr':0.001

    reward function: compute_delta_min_reward

    Experience replay:
        experience_replay/ddpg_knob_tune/tpcc_1000_2024-04-1815:57:59283850.json

    Output:
        outputs/knob_tuning_data/rl_ddpg6.json


04/18/2024 @ 4:43 PM (experiment 39)
    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}
    - iterations: 500
            'alr':0.001,
            'clr':0.001

    reward function: compute_delta_avg_reward

    Need tradeoff between latency reward and throughput reward
        - perhaps multiply throughput by some integer n?
    
    Simplify model architecture
   
    Output:
        outputs/knob_tuning_data/rl_ddpg7.json

    Observations:
        on all graphs, it seems like convergence happens around 250 - 300 iterations
        The variance between the rewards decreases considerably and becomes very tight
        unclear if avg is better than min

    Since rewards tend to be negative, multiply a positive result by some weight

    Increase size of innodb_buffer_pool_size

04/21/2024 @ 12:46 PM (experiment 40)
    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}
    - iterations: 500
            'alr':0.001,
            'clr':0.001

    - reward function: compute_delta_avg_reward
    - 'memory_size':(mem_size:=self.conn.memory_size('b')[self.database]*4),

    New knobs added:
        - innodb_adaptive_max_sleep_delay
        - eq_range_index_dive_limit
        - innodb_change_buffer_max_size
        - innodb_log_buffer_size
        - join_buffer_size
        - innodb_flushing_avg_loops

    Output:
        outputs/knob_tuning_data/rl_ddpg8.json


    Ideas:
        - increase noise_scale so as to compensate for experience replay creation
            - perhaps only decay when loss calculations begin? (when len(self.experience_replay) >= self.config['replay_size'])


    To do:
        - remove max(min((sum(k)/len(k))*10, 10), -10) clipping from compute_cost_delta_per_query
            - or apply a multiplication by 10 to the knob reward values

    Observations:
        - converged even more quickly than last time


04/21/2024 @ 2:05 PM (experiment 41)
    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}
    - iterations: 500
            'alr':0.001,
            'clr':0.001

    - reward function: compute_delta_avg_reward

    500 iterations

    Output:
        outputs/knob_tuning_data/rl_ddpg9.json

    Reproducibility check

04/21/2024 @ 3:30 PM (experiment 42, 43, and 44)
    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}
    - iterations: 500
            'alr':0.001,
            'clr':0.001

    - reward function: compute_delta_avg_reward

    300 - 400 iterations

    - removal of new knobs added

    Output:
        outputs/knob_tuning_data/rl_ddpg10.json (best)
        outputs/knob_tuning_data/rl_ddpg11.json
        outputs/knob_tuning_data/rl_ddpg12.json

04/21/2024 @ 5:52 PM (experiment 45)
    Database: tpcc100
    DQN index tuning
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.006}
    - iterations: 400
            'alr':0.0001,

    - reward function: compute_cost_delta_per_query_unscaled

    Output:
        outputs/tuning_data/rl_dqn16.json

    

04/23/2024 @ 11:56 AM (experiment 46)
    Database: tpcc_30
    DQN index tuning
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.006}
    - iterations: 400
            'alr':0.0001,

    - reward function: compute_cost_delta_per_query_unscaled

    Output:
        outputs/tuning_data/rl_dqn17.json

    Testing new database

    Seems to work!

04/23/2024 @ 12:21 PM (experiment 47)
    Database: tpcc_30
    Knob tuning
    {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}

    Output:
        outputs/knob_tuning_data/rl_ddpg13.json

    Testing new database

04/23/2024 @ 2:43 PM (experiment 48)
    Database: tpcc_30
    DQN index selection
    {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}


    compute_cost_delta_per_query_unscaled

    Test of single-agent MARL

    Output:
        outputs/tuning_data/rl_dqn19.json

    Testing new database


04/23/2024 @ 4:00 PM (experiment 49)
    Database: tpcc_30
    DQN index selection
    {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.003, 'batch_size':40, 'tpcc_time':4}

    - added a "do nothing" option

    compute_cost_delta_per_query_unscaled

    Test of single-agent MARL

    Output:
        outputs/tuning_data/rl_dqn20.json

04/23/2024 @ 4:45 PM (experiment 50)
    Database: tpcc_30
    Knob tuning
    {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4, 'marl_step':50}

    200 iterations

    Test of single-agent MARL

    Output:
        outputs/knob_tuning_data/rl_ddpg14.json

04/23/2024 @ 6:45 PM (experiment 51)
    Database: tpcc_30
    Both index and knob tuning

    200 iterations

    Index tuner:
        {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003, 'tpcc_time':4, 'marl_step':50}

    Knob tuner:
        {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4, 'marl_step':50}


    compute_team_reward_avg

    Output:
        outputs/marl_tuning_data/marl1.json
        experience_replay/dqn_index_tune/experience_replay_tpcc_30_2024-04-2318:44:44768074.json
