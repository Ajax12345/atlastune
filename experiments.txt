
06/04/2024 @ 6:28 PM
    - Currently plan on trying TPC-C with all primary indices dropped.
    - If this does not work on standard model, will switch to TPC-H
    - Might want to run installation of TPC-H again, we cleared out the large text fields for the sake of memory. It might not matter, though.


06/08/2024 @ 12:20 PM
    - 500 iterations
    - idea: change actor/critic network architecture
        - add more layers
        - add fewer?
    - try with larger batch size
    - see that only a small number of -20 rewards occur
    - there is a poor solving gradient: assuming you converge on index i, which is optimal, then anything better will simply need to a negative result, since it has plateued and cannot find a better solution
        - trick might be to check performance of new candidate relative to any of the existing solutions in the memory buffer
        - should be measured against the best seen so far, at any point
        - new reward:
            - number of candidates it is better than?
            - number of better solutions, number of worse solutions?
            - in terms of total cost

    - TODO: reward is -1*query_cost
        - might need to normalize query_cost
    - Also try the reward as being -1*execution time    
    - Try other reward formulas
    - detect convergence: that is, when suggested solutions are not noticeable better than the others seen, after a certain period of time
    - if -1*query_cost or -1*execution_time does not work, then get baseline costs for workload and then find max(original - new for original in baselines)
    - check that weight updating is correct
    - try on TPC-H
    - repurpose old reward function based on percentage change, but the w1 is the original (first) non-indexed state

06/09/2024 @ 11:29 AM
    reward function is now -1*total_query_cost
    experience replay: custom_exprience_replay2024-04-0911:36:45523674.json
    replay batch size: 50
    weight updates: target * t + old* (1 - t)
    reward function: compute_total_cost_reward

    - Observations:
        - total cost too large, had to normalize rewards, but loss did not decrease

    - TODO:
        - change weight updates to be: target * (1 - t) + old * t
        - have reward be the difference between default configuration and proposed configuration
            - clip at -10 and 10


06/09/2024 @ 11:58 AM
    reward now  difference between default configuration and proposed configuration
    reward function: compute_cost_delta
    weight updates: target * t + old* (1 - t)
    
    Oberservations:
        - loss does not decrease
        - 99% of the rewards were = 5


06/09/2024 @ 1:22 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json

    ideas:
        plot size of reward given out at each iteration
        change around critic and actor architecture
        pass to critic the activated action values
        reward function does not just approximate from experience_replay[0], but a random sample of prior solutions
        using incorrect loss function?
        - try on TPC-H

    - TODO: plot returned reward at each iteration
        - initialize weights differently (see papers)
            https://www.reddit.com/r/reinforcementlearning/comments/ookni2/sac_critic_loss_increases/
        - decrease the size of the critic network

        - https://stackoverflow.com/questions/58004237/critic-loss-for-rl-agent

06/09/2024 @ 2:00 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json

    plotting rewards per iteration

    using _init_weights

    observations:
        reward per iteration increasing!!!

    TODO:
        - remove _init_weights
        - try on 500 iterations
        - try on TPC-H
        - update reward function to sample from elsewere in the experience replay
        - decrease the size of the critic network
        - train over 5 epochs, find the mean of each reward at every iteration
        - compare with purely random search

06/09/2024 @ 3:57 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json

    300 iterations over five epochs


    Observation:    
        - logarithmic increase

     TODO: 
        - run purely random over five epochs

    - Try DQN?
    - Implement this algo more closely:
        - https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode
        - differentiate between Q(s, a) and Q(s, u(s))

06/09/2024 @ 6:09 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json
    
    - using both Q(s, a) and Q(s, u(s))
    - target_param.data * self.config['tau'] + param.data * (1-self.config['tau'])
    - epsilon = 0.75
    - batch_size = 20

    Ideas
        - have bound = 2 in generate_random_index
        - change network layers
        - possible overfitting? try again with the tuner context manager inside the loop
        - increase batch size


06/09/2024 @ 7:49 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json
    true epoch tuning

    Observations: consistent growth

    Output: outputs/rl_ddpg.json

    Ideas:
        - normalize reward
    

06/09/2024 @ 8:36 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json
    
    100 iterations

    Output: outputs/rl_ddpg1.json

    - normalization of batch rewards
        - Observation: no major difference between reward normalization and no normalization
        - a little more stable, but not by much

    Ideas:
        - reward could be original_total_cost - new_total_cost, normalized across batch
        - reduce network size
        - new reward functions
        - try on TPC-H
        - reward function could be based on query execution time
        - try DQN

06/09/2024 @ 9:48 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json
    
    Output: outputs/rl_ddpg2.json

    Same as last experiement, but over 300 iterations

    Cannot break past the reward threshold of six


06/10/2024 @ 10:46 AM
    -1*predicted_q_value

    Output: outputs/rl_ddpg3.json
    more performant than previously: breaks threshold of six

    TODO: 
        - remove epsilon greedy
        - simplify network models

06/10/2024 @ 11:39 AM
    -1*predicted_q_value

    removed epsilon greedy

    Output: outputs/rl_ddpg4.json

    Epsilon greedy fails: 
        - plateaus at 6.3

    TODO: 
        - add noise to actor in place of epsilon greedy
        - simplify network models

        put weight updates in body of with torch.no_grad():



06/10/2024 @ 11:59 AM
    -1*predicted_q_value

    Output: outputs/rl_ddpg5.json
    
    put weight updates in body of with torch.no_grad():

    Oberservations:
        - poorer performance

    TODO: 
        - add noise to actor in place of epsilon greedy
            - np.random.randn(5)
        - simplify network models


06/10/2024 @ 12:29 PM
    -1*predicted_q_value

    Output: outputs/rl_ddpg6.json
            outputs/rl_ddpg7.json
            outputs/rl_ddpg8.json
            outputs/rl_ddpg9.json

    
    Adding actor noise in place of epsilon greedy
        noise_scale: 0.25

    TODO: 
        - simplify network models


06/11/2024 @ 9:30 AM
    -1*predicted_q_value

    Output: outputs/rl_ddpg10.json
            
    Trying to decrease volatility

    noise_scale: 0.05
        - no meaningful volatility decrease

    TODO: 
        - simplify network models
        - update rewards so that change of 0 is not included in the average

06/11/2024 @ 10:00 AM

    Output: outputs/rl_ddpg11.json
            outputs/rl_ddpg12.json

    Simplified network architecture
        
    Trying to decrease volatility

    noise_scale: 0.05

    Oberservations:
        - approaches 8
        - BEST DDPG TO DATE
        - needs to be run over more iterations to confirm, though

    TODO: 
        - update rewards so that change of 0 is not included in the average
        - could also subtract -4 from each reward to add more of a penalty and encourage further exploration
        - may need to run on larger databases for a better problem gradient
        - another possible reward function:
            r = (cost(workload, i) - cost(workload, i+1))/cost(workload, 0)
        
        - penalize if agent chooses an index that is not used in any expression?
        - in DQN, copy weights after long time lapses

06/11/2024 @ 7:49 PM

    Output: outputs/rl_ddpg14.json

    Normalizing index configuration and metrics

    Observations:
        - no meaningful change


06/12/2024 @ 11:57 AM
    Using DQN

    Experience replay:
        experience_replay/dqn_index_tune/experience_replay_tpcc100_2024-04-1212:00:36529360.json


    'weight_copy_interval':20
    'epsilon':0.7,
    'epsilon_decay':0.01,
    'batch_sample_size':30


    Output: outputs/rl_dqn1.json

    Observations:
        clearly steady learning!!

    TODO: 
        - update rewards so that change of 0 is not included in the average
        - could also subtract -4 from each reward to add more of a penalty and encourage further exploration
        - may need to run on larger databases for a better problem gradient
        - another possible reward function:
            r = (cost(workload, i) - cost(workload, i+1))/cost(workload, 0)
        
        - penalize if agent chooses an index that is not used in any expression?
        - in DQN, copy weights after long time lapses
        - try Huber Loss
        - train over multiple different epochs
        - need more exploration:
            - annealing should either be removed or cooling factor significantly decreased
        
        - decrease learning rate


06/12/2024 @ 2:19 PM
    Using DQN

    Four training runs for mean calculation
    
    Experience replay:
        experience_replay/dqn_index_tune/experience_replay_tpcc100_2024-04-1212:00:36529360.json

    Output: 
        outputs/rl_dqn2.json
        outputs/rl_dqn3.json
        outputs/rl_dqn4.json

    'lr':0.0001,
    'gamma':0.9,
    'weight_copy_interval':10,
    'tau':0.9999,
    'epsilon':1,
    'epsilon_decay':0.001,
    'batch_sample_size':50

    Observations:
        - need more exploration
            - cooling factor needs to be much smaller


06/12/2024 @ 3:40 PM
    DQN

    hard-coded e = 0.4

    no substantial growth beyond 6


06/12/2024 @ 3:46 PM
    DQN

    Experience replay: experience_replay/dqn_index_tune/experience_replay_tpcc100_2024-04-1212:00:36529360.json
    - {'epsilon':1, 'epsilon_decay':0.004}
    - 500 iterations
    - 4 epochs
    
    Output: rl_dqn6.json

    Observations:
        converges around 5

06/12/2024 @ 4:01 PM
    DQN

    - Generates new experience replay every time
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.001}
    - 500 iterations
    - 4 epochs

    Output: outputs/rl_dqn7.json

    Observations:
        - converges to around 6

    Ideas:
        - Need to experiment with annealing, weight_copy_interval, etc
        - Also need epochs for training that preserve models, optimizer, etc. 


06/12/2024 @ 8:22 PM

    DQN

    - running 5 epochs without resetting models
    - initial memory replay size = 150, reset at every epoch
    - 500 iterations per epoch

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.001}

    Output:
        outputs/tuning_data/rl_dqn2.json