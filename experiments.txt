
06/04/2024 @ 6:28 PM
    - Currently plan on trying TPC-C with all primary indices dropped.
    - If this does not work on standard model, will switch to TPC-H
    - Might want to run installation of TPC-H again, we cleared out the large text fields for the sake of memory. It might not matter, though.


06/08/2024 @12:20 PM
    - 500 iterations
    - idea: change actor/critic network architecture
        - add more layers
        - add fewer?
    - try with larger batch size
    - see that only a small number of -20 rewards occur
    - there is a poor solving gradient: assuming you converge on index i, which is optimal, then anything better will simply need to a negative result, since it has plateued and cannot find a better solution
        - trick might be to check performance of new candidate relative to any of the existing solutions in the memory buffer
        - should be measured against the best seen so far, at any point
        - new reward:
            - number of candidates it is better than?
            - number of better solutions, number of worse solutions?
            - in terms of total cost

    - TODO: reward is -1*query_cost
        - might need to normalize query_cost
    - Also try the reward as being -1*execution time    
    - Try other reward formulas
    - detect convergence: that is, when suggested solutions are not noticeable better than the others seen, after a certain period of time
    - if -1*query_cost or -1*execution_time does not work, then get baseline costs for workload and then find max(original - new for original in baselines)
    - check that weight updating is correct
    - try on TPC-H
    - repurpose old reward function based on percentage change, but the w1 is the original (first) non-indexed state

06/09/2024 @11:29 AM
    reward function is now -1*total_query_cost
    experience replay: custom_exprience_replay2024-04-0911:36:45523674.json
    replay batch size: 50
    weight updates: target * t + old* (1 - t)
    reward function: compute_total_cost_reward

    - Observations:
        - total cost too large, had to normalize rewards, but loss did not decrease

    - TODO:
        - change weight updates to be: target * (1 - t) + old * t
        - have reward be the difference between default configuration and proposed configuration
            - clip at -10 and 10


06/09/2024 @11:58 AM
    reward now  difference between default configuration and proposed configuration
    reward function: compute_cost_delta
    weight updates: target * t + old* (1 - t)
    
    Oberservations:
        - loss does not decrease
        - 99% of the rewards were = 5


06/09/2024 @1:22 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json

    ideas:
        plot size of reward given out at each iteration
        change around critic and actor architecture
        pass to critic the activated action values
        reward function does not just approximate from experience_replay[0], but a random sample of prior solutions
        using incorrect loss function?
        - try on TPC-H

    - TODO: plot returned reward at each iteration
        - initialize weights differently (see papers)
            https://www.reddit.com/r/reinforcementlearning/comments/ookni2/sac_critic_loss_increases/
        - decrease the size of the critic network

        - https://stackoverflow.com/questions/58004237/critic-loss-for-rl-agent

06/09/2024 @2:00 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json

    plotting rewards per iteration

    using _init_weights

    observations:
        reward per iteration increasing!!!

    TODO:
        - remove _init_weights
        - try on 500 iterations
        - try on TPC-H
        - update reward function to sample from elsewere in the experience replay
        - decrease the size of the critic network
        - train over 5 epochs, find the mean of each reward at every iteration
        - compare with purely random search

06/09/2024 @3:57 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json

    300 iterations over five epochs


    Observation:    
        - logarithmic increase

     TODO: 
        - run purely random over five epochs

    - Try DQN?
    - Implement this algo more closely:
        - https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode
        - differentiate between Q(s, a) and Q(s, u(s))