
04/04/2024 @ 6:28 PM
    - Currently plan on trying TPC-C with all primary indices dropped.
    - If this does not work on standard model, will switch to TPC-H
    - Might want to run installation of TPC-H again, we cleared out the large text fields for the sake of memory. It might not matter, though.


04/08/2024 @ 12:20 PM
    - 500 iterations
    - idea: change actor/critic network architecture
        - add more layers
        - add fewer?
    - try with larger batch size
    - see that only a small number of -20 rewards occur
    - there is a poor solving gradient: assuming you converge on index i, which is optimal, then anything better will simply need to a negative result, since it has plateued and cannot find a better solution
        - trick might be to check performance of new candidate relative to any of the existing solutions in the memory buffer
        - should be measured against the best seen so far, at any point
        - new reward:
            - number of candidates it is better than?
            - number of better solutions, number of worse solutions?
            - in terms of total cost

    - TODO: reward is -1*query_cost
        - might need to normalize query_cost
    - Also try the reward as being -1*execution time    
    - Try other reward formulas
    - detect convergence: that is, when suggested solutions are not noticeable better than the others seen, after a certain period of time
    - if -1*query_cost or -1*execution_time does not work, then get baseline costs for workload and then find max(original - new for original in baselines)
    - check that weight updating is correct
    - try on TPC-H
    - repurpose old reward function based on percentage change, but the w1 is the original (first) non-indexed state

04/09/2024 @ 11:29 AM
    reward function is now -1*total_query_cost
    experience replay: custom_exprience_replay2024-04-0911:36:45523674.json
    replay batch size: 50
    weight updates: target * t + old* (1 - t)
    reward function: compute_total_cost_reward

    - Observations:
        - total cost too large, had to normalize rewards, but loss did not decrease

    - TODO:
        - change weight updates to be: target * (1 - t) + old * t
        - have reward be the difference between default configuration and proposed configuration
            - clip at -10 and 10


04/09/2024 @ 11:58 AM
    reward now  difference between default configuration and proposed configuration
    reward function: compute_cost_delta
    weight updates: target * t + old* (1 - t)
    
    Oberservations:
        - loss does not decrease
        - 99% of the rewards were = 5


04/09/2024 @ 1:22 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json

    ideas:
        plot size of reward given out at each iteration
        change around critic and actor architecture
        pass to critic the activated action values
        reward function does not just approximate from experience_replay[0], but a random sample of prior solutions
        using incorrect loss function?
        - try on TPC-H

    - TODO: plot returned reward at each iteration
        - initialize weights differently (see papers)
            https://www.reddit.com/r/reinforcementlearning/comments/ookni2/sac_critic_loss_increases/
        - decrease the size of the critic network

        - https://stackoverflow.com/questions/58004237/critic-loss-for-rl-agent

04/09/2024 @ 2:00 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json

    plotting rewards per iteration

    using _init_weights

    observations:
        reward per iteration increasing!!!

    TODO:
        - remove _init_weights
        - try on 500 iterations
        - try on TPC-H
        - update reward function to sample from elsewere in the experience replay
        - decrease the size of the critic network
        - train over 5 epochs, find the mean of each reward at every iteration
        - compare with purely random search

04/09/2024 @ 3:57 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json

    300 iterations over five epochs


    Observation:    
        - logarithmic increase

     TODO: 
        - run purely random over five epochs

    - Try DQN?
    - Implement this algo more closely:
        - https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode
        - differentiate between Q(s, a) and Q(s, u(s))

04/09/2024 @ 6:09 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json
    
    - using both Q(s, a) and Q(s, u(s))
    - target_param.data * self.config['tau'] + param.data * (1-self.config['tau'])
    - epsilon = 0.75
    - batch_size = 20

    Ideas
        - have bound = 2 in generate_random_index
        - change network layers
        - possible overfitting? try again with the tuner context manager inside the loop
        - increase batch size


04/09/2024 @ 7:49 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json
    true epoch tuning

    Observations: consistent growth

    Output: outputs/rl_ddpg.json

    Ideas:
        - normalize reward
    

04/09/2024 @ 8:36 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json
    
    100 iterations

    Output: outputs/rl_ddpg1.json

    - normalization of batch rewards
        - Observation: no major difference between reward normalization and no normalization
        - a little more stable, but not by much

    Ideas:
        - reward could be original_total_cost - new_total_cost, normalized across batch
        - reduce network size
        - new reward functions
        - try on TPC-H
        - reward function could be based on query execution time
        - try DQN

04/09/2024 @ 9:48 PM
    reward function: compute_cost_delta_per_query
    experience replay: experience_replay/custom_exprience_replay2024-04-0913:18:31438505.json
    
    Output: outputs/rl_ddpg2.json

    Same as last experiement, but over 300 iterations

    Cannot break past the reward threshold of six


04/10/2024 @ 10:46 AM
    -1*predicted_q_value

    Output: outputs/rl_ddpg3.json
    more performant than previously: breaks threshold of six

    TODO: 
        - remove epsilon greedy
        - simplify network models

04/10/2024 @ 11:39 AM
    -1*predicted_q_value

    removed epsilon greedy

    Output: outputs/rl_ddpg4.json

    Epsilon greedy fails: 
        - plateaus at 6.3

    TODO: 
        - add noise to actor in place of epsilon greedy
        - simplify network models

        put weight updates in body of with torch.no_grad():



04/10/2024 @ 11:59 AM
    -1*predicted_q_value

    Output: outputs/rl_ddpg5.json
    
    put weight updates in body of with torch.no_grad():

    Oberservations:
        - poorer performance

    TODO: 
        - add noise to actor in place of epsilon greedy
            - np.random.randn(5)
        - simplify network models


04/10/2024 @ 12:29 PM
    -1*predicted_q_value

    Output: outputs/rl_ddpg6.json
            outputs/rl_ddpg7.json
            outputs/rl_ddpg8.json
            outputs/rl_ddpg9.json

    
    Adding actor noise in place of epsilon greedy
        noise_scale: 0.25

    TODO: 
        - simplify network models


04/11/2024 @ 9:30 AM
    -1*predicted_q_value

    Output: outputs/rl_ddpg10.json
            
    Trying to decrease volatility

    noise_scale: 0.05
        - no meaningful volatility decrease

    TODO: 
        - simplify network models
        - update rewards so that change of 0 is not included in the average

04/11/2024 @ 10:00 AM

    Output: outputs/rl_ddpg11.json
            outputs/rl_ddpg12.json

    Simplified network architecture
        
    Trying to decrease volatility

    noise_scale: 0.05

    Oberservations:
        - approaches 8
        - BEST DDPG TO DATE
        - needs to be run over more iterations to confirm, though

    TODO: 
        - update rewards so that change of 0 is not included in the average
        - could also subtract -4 from each reward to add more of a penalty and encourage further exploration
        - may need to run on larger databases for a better problem gradient
        - another possible reward function:
            r = (cost(workload, i) - cost(workload, i+1))/cost(workload, 0)
        
        - penalize if agent chooses an index that is not used in any expression?
        - in DQN, copy weights after long time lapses

04/11/2024 @ 7:49 PM

    Output: outputs/rl_ddpg14.json

    Normalizing index configuration and metrics

    Observations:
        - no meaningful change


04/12/2024 @ 11:57 AM
    Using DQN

    Experience replay:
        experience_replay/dqn_index_tune/experience_replay_tpcc100_2024-04-1212:00:36529360.json


    'weight_copy_interval':20
    'epsilon':0.7,
    'epsilon_decay':0.01,
    'batch_sample_size':30


    Output: outputs/rl_dqn1.json

    Observations:
        clearly steady learning!!

    TODO: 
        - update rewards so that change of 0 is not included in the average
        - could also subtract -4 from each reward to add more of a penalty and encourage further exploration
        - may need to run on larger databases for a better problem gradient
        - another possible reward function:
            r = (cost(workload, i) - cost(workload, i+1))/cost(workload, 0)
        
        - penalize if agent chooses an index that is not used in any expression?
        - in DQN, copy weights after long time lapses
        - try Huber Loss
        - train over multiple different epochs
        - need more exploration:
            - annealing should either be removed or cooling factor significantly decreased
        
        - decrease learning rate


04/12/2024 @ 2:19 PM
    Using DQN

    Four training runs for mean calculation
    
    Experience replay:
        experience_replay/dqn_index_tune/experience_replay_tpcc100_2024-04-1212:00:36529360.json

    Output: 
        outputs/rl_dqn2.json
        outputs/rl_dqn3.json
        outputs/rl_dqn4.json

    'lr':0.0001,
    'gamma':0.9,
    'weight_copy_interval':10,
    'tau':0.9999,
    'epsilon':1,
    'epsilon_decay':0.001,
    'batch_sample_size':50

    Observations:
        - need more exploration
            - cooling factor needs to be much smaller


04/12/2024 @ 3:40 PM
    DQN

    hard-coded e = 0.4

    no substantial growth beyond 6


04/12/2024 @ 3:46 PM
    DQN

    Experience replay: experience_replay/dqn_index_tune/experience_replay_tpcc100_2024-04-1212:00:36529360.json
    - {'epsilon':1, 'epsilon_decay':0.004}
    - 500 iterations
    - 4 epochs
    
    Output: rl_dqn6.json

    Observations:
        converges around 5

04/12/2024 @ 4:01 PM
    DQN

    - Generates new experience replay every time
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.001}
    - 500 iterations
    - 4 epochs

    Output: outputs/rl_dqn7.json

    Observations:
        - converges to around 6

    Ideas:
        - Need to experiment with annealing, weight_copy_interval, etc
        - Also need epochs for training that preserve models, optimizer, etc. 


04/12/2024 @ 8:22 PM

    DQN

    - running 5 epochs without resetting models
    - initial memory replay size = 150, reset at every epoch
    - 500 iterations per epoch

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.001}

    Output:
        outputs/tuning_data/rl_dqn2.json


04/13/2024 @ 11:36 AM (experiment 25)
    DQN

    - running 5 individual epochs (model resetting)
    - replay memory size: 150
        - new experience replay created at each epoch
    
    - 500 iterations per epoch
    - includes new delta cost metric
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.006}
    - reward_func = 'compute_total_cost_reward'

    Output: 
        outputs/tuning_data/rl_dqn3_5.json

    Observations:
        - need to reduce epsilon decay further (try 0.003)
    

04/13/2024 @ 12:06 PM (experiment 26)
    DQN

    - Same as experiment 25, except:
        {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.0025}
    
    Output: 
        outputs/tuning_data/rl_dqn3.json

    Ideas:
        - increase batch sample size
        - increase number of iterations before weights are swapped

        Possible reward functions:
            (cost(0, workload) - cost(i+1, workload))/cost(0, workload) + (cost(i, workload) - cost(i+1, workload))/cost(i, workload)

            if cost(0, workload) > cost(i+1, workload):
                if cost(i, workload) > cost(i+1, workload):
                    return 10
                return 5

            return -5


            (cost(workload, i) - cost(workload, i+1))/cost(workload, 0)

            (min(costs[j:k]) - cost(i+1, workload))/min(costs[j:k])


04/13/2024 @ 7:28 PM (experiment 27)
    DQN
    Four epochs, 500 iterations

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.0025}
    
    reward function: compute_min_step_cost

    Output: 
        outputs/tuning_data/rl_dqn4.json




04/13/2024 @ 7:28 PM (experiment 28)
    DQN
    Four epochs, 500 iterations

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.0025}
    
    reward function: compute_total_cost_reward

    Trying total cost reward with a smaller epsilon decay value to see if results from experiment 25 can be replicated

    Output: 
        outputs/tuning_data/rl_dqn5.json


04/13/2024 @ 8:50 PM (experiment 28_1)
    DQN
    Four epochs, 500 iterations
    Same as experiment 28, attempting to replicate results

    Output: 
        outputs/tuning_data/rl_dqn6.json


04/13/2024 @ 8:50 PM (experiment 29)
    DQN
    Four epochs, 500 iterations
    Same as experiment 28, attempting to replicate results

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.007}

    compute_total_cost_reward
    
    Output: 
        outputs/tuning_data/rl_dqn7.json

04/13/2024 @ 9:52 PM (experiment 30)
    DQN
    Four epochs, 500 iterations
    Same as experiment 28, attempting to replicate results

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003}

    compute_increment_step_cost
    
    Output: 
        outputs/tuning_data/rl_dqn8.json

    Some interesting growth. Experiment should be run again to see if results can be replicated

    Currently the best configuration



04/14/2024 @ 10:25 AM (experiment 31)
    DQN

    Q-value action layer now has length of len(indices) + 1
        - corresponding to a "do nothing" action

    reward function: compute_cost_delta_per_query

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003}

    Output:
        outputs/tuning_data/rl_dqn10.json

    Observations:
        no meaningful impact


04/14/2024 @ 10:48 AM (experiment 32)
    DQN

    Running on TPCH1

    reward function: compute_cost_delta_per_query

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003}

    Output:
        outputs/tuning_data/rl_dqn11.json


    A bit more volatile


04/14/2024 @ 12:43 PM (experiment 33)
    DQN

    Running on TPCH1

    reward function: compute_total_cost_reward

    {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.006}

    Output:
        outputs/tuning_data/rl_dqn12.json

    Ideas:
        - add "no action" option
        - run over 1000 iterations

    Observations:
        - compute_total_cost_reward seemingly effective
        - could be normalized by dividing by the initial total cost
        - https://news.ycombinator.com/item?id=40028111
        - Would not the query cost calculations consider available disk space, etc?

    Questions:
        - weight_copy_interval size?
        - batch size?
        - experience_replay size?
    

04/16/2024 @ 4:00 PM (experiment 34)
    - DQN
    - running on sysbench_tpcc
        - scale = 25

    - 4 epochs, 400 iterations
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.006}

    Output:
        outputs/tuning_data/rl_dqn14.json

    Ideas:
        - can probably scale up sysbench_tpcc to scale = 50
        - instead of initial index configuration as part of the state, vectorize the query payload

04/16/2024 @ 9:00 PM (experiment 35)
    - DQN
    - running on tpcc_1000

    - 4 epochs, 400 iterations
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.006}

    Output:
        outputs/tuning_data/rl_dqn15.json


######################### KNOB TUNING #########################

04/18/2024 @ 4:25 PM (experiment 36)
    - DDPG
    - Might need to increase size of min_memory in knobs to be the default on my system
    - need delay between apply_knob_config and tpcc_metrics?
        - IMPORTANT: verify sequential action here
    - for training only knob tuning, remove indices from the state 
    - should "selected_action" be saved in memory replay without noise added to it?

    reward function: compute_delta_min_reward

    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':0.5, 'noise_decay':0.001, 'batch_size':20, 'tpcc_time':4}
    - iterations: 100
            'alr':0.001,
            'clr':0.001

    Ideas:
        - take average of rewards instead of min
        - pass larger time value to tpcc_metrics
        - #TODO: increase noise_scale for more exploration
        - verify that knobs being used are useful
        - increase database size
        - use sysbench_tpcc
        - use cdbtune's reward function
    
    Knobs to use:
        - innodb_adaptive_flushing_lwm (0, 70)
        - innodb_adaptive_max_sleep_delay *** (0, 1000000)
        - innodb_change_buffering *** ['none', 'inserts', 'deletes', 'changes', 'purges', 'all']
        - innodb_max_dirty_pages_pct [0, 99, 75]
        - innodb_max_dirty_pages_pct_lwm [0, 99, 0]
        - max_heap_table_size [16384, memory_size*5]
        - binlog_cache_size
        - binlog_max_flush_queue_time
        - eq_range_index_dive_limit *** [0, 10000, 200]
        - host_cache_size ** [0, 65536]
        - innodb_adaptive_flushing *
        - innodb_autoextend_increment
        - innodb_buffer_pool_load_at_startup
        - innodb_change_buffer_max_size ***
        - innodb_commit_concurrency **
        - innodb_concurrency_tickets ** [1, 4294967295]
        - innodb_flush_log_at_timeout **
        - innodb_flushing_avg_loops ***
        - innodb_ft_sort_pll_degree [1, 16]
        - innodb_log_buffer_size *** [1048576, 4294967295]
        - innodb_max_purge_lag *
        - innodb_old_blocks_pct **
        - innodb_old_blocks_time **
        - innodb_rollback_segments *
        - innodb_ddl_buffer_size *** [65536, 4294967295]
        - join_buffer_size ***
        - max_binlog_cache_size *?
        - max_binlog_size
        - max_seeks_for_key **
        - max_write_lock_count *
        - innodb_lock_wait_timeout [1, 	1073741824]
        - query_alloc_block_size **

    Important knobs: query_cache_size, innodb_buffer_pool_size, join_buffer_size, eq_range_index_dive_limit


    Experience replay:
        experience_replay/ddpg_knob_tune/tpcc_1000_2024-04-1815:57:59283850.json

    Output:
        outputs/knob_tuning_data/rl_ddpg4.json


04/18/2024 @ 4:43 PM (experiment 37)
    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':0.8, 'noise_decay':0.004, 'batch_size':20, 'tpcc_time':4}
    - iterations: 500
            'alr':0.001,
            'clr':0.001

    reward function: compute_delta_min_reward

    Experience replay:
        experience_replay/ddpg_knob_tune/tpcc_1000_2024-04-1815:57:59283850.json

    Output:
        outputs/knob_tuning_data/rl_ddpg5.json

    Observations: might be overshooting: try smaller learning rate

04/18/2024 @ 4:43 (experiment 38)
    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}
    - iterations: 500
            'alr':0.001,
            'clr':0.001

    reward function: compute_delta_min_reward

    Experience replay:
        experience_replay/ddpg_knob_tune/tpcc_1000_2024-04-1815:57:59283850.json

    Output:
        outputs/knob_tuning_data/rl_ddpg6.json


04/18/2024 @ 4:43 PM (experiment 39)
    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}
    - iterations: 500
            'alr':0.001,
            'clr':0.001

    reward function: compute_delta_avg_reward

    Need tradeoff between latency reward and throughput reward
        - perhaps multiply throughput by some integer n?
    
    Simplify model architecture
   
    Output:
        outputs/knob_tuning_data/rl_ddpg7.json

    Observations:
        on all graphs, it seems like convergence happens around 250 - 300 iterations
        The variance between the rewards decreases considerably and becomes very tight
        unclear if avg is better than min

    Since rewards tend to be negative, multiply a positive result by some weight

    Increase size of innodb_buffer_pool_size

04/21/2024 @ 12:46 PM (experiment 40)
    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}
    - iterations: 500
            'alr':0.001,
            'clr':0.001

    - reward function: compute_delta_avg_reward
    - 'memory_size':(mem_size:=self.conn.memory_size('b')[self.database]*4),

    New knobs added:
        - innodb_adaptive_max_sleep_delay
        - eq_range_index_dive_limit
        - innodb_change_buffer_max_size
        - innodb_log_buffer_size
        - join_buffer_size
        - innodb_flushing_avg_loops

    Output:
        outputs/knob_tuning_data/rl_ddpg8.json


    Ideas:
        - increase noise_scale so as to compensate for experience replay creation
            - perhaps only decay when loss calculations begin? (when len(self.experience_replay) >= self.config['replay_size'])


    To do:
        - remove max(min((sum(k)/len(k))*10, 10), -10) clipping from compute_cost_delta_per_query
            - or apply a multiplication by 10 to the knob reward values

    Observations:
        - converged even more quickly than last time


04/21/2024 @ 2:05 PM (experiment 41)
    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}
    - iterations: 500
            'alr':0.001,
            'clr':0.001

    - reward function: compute_delta_avg_reward

    500 iterations

    Output:
        outputs/knob_tuning_data/rl_ddpg9.json

    Reproducibility check

04/21/2024 @ 3:30 PM (experiment 42, 43, and 44)
    Database: tpcc_1000
    - {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}
    - iterations: 500
            'alr':0.001,
            'clr':0.001

    - reward function: compute_delta_avg_reward

    300 - 400 iterations

    - removal of new knobs added

    Output:
        outputs/knob_tuning_data/rl_ddpg10.json (best)
        outputs/knob_tuning_data/rl_ddpg11.json
        outputs/knob_tuning_data/rl_ddpg12.json

04/21/2024 @ 5:52 PM (experiment 45)
    Database: tpcc100
    DQN index tuning
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.006}
    - iterations: 400
            'alr':0.0001,

    - reward function: compute_cost_delta_per_query_unscaled

    Output:
        outputs/tuning_data/rl_dqn16.json

    

04/23/2024 @ 11:56 AM (experiment 46)
    Database: tpcc_30
    DQN index tuning
    - {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.006}
    - iterations: 400
            'alr':0.0001,

    - reward function: compute_cost_delta_per_query_unscaled

    Output:
        outputs/tuning_data/rl_dqn17.json

    Testing new database

    Seems to work!

04/23/2024 @ 12:21 PM (experiment 47)
    Database: tpcc_30
    Knob tuning
    {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}

    Output:
        outputs/knob_tuning_data/rl_ddpg13.json

    Testing new database

04/23/2024 @ 2:43 PM (experiment 48)
    Database: tpcc_30
    DQN index selection
    {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4}


    compute_cost_delta_per_query_unscaled

    Test of single-agent MARL

    Output:
        outputs/tuning_data/rl_dqn19.json

    Testing new database


04/23/2024 @ 4:00 PM (experiment 49)
    Database: tpcc_30
    DQN index selection
    {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.003, 'batch_size':40, 'tpcc_time':4}

    - added a "do nothing" option

    compute_cost_delta_per_query_unscaled

    Test of single-agent MARL

    Output:
        outputs/tuning_data/rl_dqn20.json

04/23/2024 @ 4:45 PM (experiment 50)
    Database: tpcc_30
    Knob tuning
    {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4, 'marl_step':50}

    200 iterations

    Test of single-agent MARL

    Output:
        outputs/knob_tuning_data/rl_ddpg14.json

04/23/2024 @ 6:45 PM (experiment 51)
    Database: tpcc_30
    Both index and knob tuning

    200 iterations

    Index tuner:
        {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003, 'tpcc_time':4, 'marl_step':50}

    Knob tuner:
        {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4, 'marl_step':50}


    compute_team_reward_avg

    Output:
        outputs/marl_tuning_data/marl1.json
        experience_replay/dqn_index_tune/experience_replay_tpcc_30_2024-04-2318:44:44768074.json


04/23/2024 @ 12:48 PM (experiment 52)
    Database: tpcc_30
    Both index and knob tuning

    200 iterations

    Index tuner:
        {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003, 'tpcc_time':4, 'marl_step':50}

    Knob tuner:
        {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4, 'marl_step':50}


    compute_team_reward_scaled

    Output:
        outputs/marl_tuning_data/marl3.json
        experience_replay/dqn_index_tune/experience_replay_tpcc_30_2024-04-2412:29:18214111.json



    Idea:
        - don't have team rewards?
        - simply train on shared states?


04/23/2024 @ 6:30 PM (experiment 53)
    Database: tpcc_30
    Both index and knob tuning

    300 iterations

    Index tuner:
        {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003, 'tpcc_time':4, 'marl_step':50}

    Knob tuner:
        {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4, 'marl_step':50}


    Back to compute_cost_delta_per_query_unscaled and compute_team_reward_min

    Output:
        outputs/marl_tuning_data/marl4.json
        experience_replay/dqn_index_tune/experience_replay_tpcc_30_2024-04-2417:56:31942583.json


    Idea:
        - don't have team rewards?
        - simply train on shared states?
        - use team reward function, but deprioritize the values of the other components via a weight
            index selection reward:
                computed_workload_cost + 0.2*latency + 0.1*throughput

            knob tuning reward:
                avg(latency, throughput) + 0.2*computed_workload_cost

04/27/2024 @ 1:21 PM (experiment 54)
    Database: tpcc_30
    Both index and knob tuning

    300 iterations

    Index tuner:
        {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003, 'tpcc_time':4, 'marl_step':50}

    Knob tuner:
        {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4, 'marl_step':50}

    compute_cost_delta_per_query_unscaled
    compute_delta_avg_reward

    Output:
        outputs/marl_tuning_data/marl5.json
        outputs/marl_tuning_data/marl6.json
        outputs/marl_tuning_data/marl7.json
        outputs/marl_tuning_data/marl8.json
        outputs/marl_tuning_data/marl9.json
        Graph: experiment_54_agg.png


04/27/2024 @ 11:08 AM (experiment 55)
    Database: tpcc_30
    Both index and knob tuning

    Non-MARL

    300 iterations

    Index tuner:
        {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003, 'tpcc_time':4, 'marl_step':50}

    Knob tuner:
        {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4, 'marl_step':50}

    compute_cost_delta_per_query_unscaled
    compute_delta_avg_reward

    Output:
        outputs/marl_tuning_data/marl11.json
        outputs/marl_tuning_data/marl12.json

    Experience replay: experience_replay/dqn_index_tune/experience_replay_tpcc_30_2024-04-2914:23:35380520.json


    Ideas:
        - Need to train both agents on the same workload
            - this could be either the custom index selection query set or TPC-C
                - might want to use TPC-H

            - If using custom read workload:
                - remove knobs that do not contribute to read functionality?
            
            - Use only TPC-C workload
                - have index selection reward function be the same as the one used for knobs

            - Can continue double workload paradignm, but change index selection workload to be:
                computed_workload_cost + 0.4*latency + 0.4*throughput
                or min(computed_workload_cost, latency, throughput)
                 
                - and keep knob tuner reward function the same

            - Instead of interleaving, run both to convergence sequentially 

            - https://github.com/tvondra/pg_tpch
            - https://github.com/catarinaribeir0/queries-tpch-dbgen-mysql
            - https://github.com/acshulyak/mysql_tpch
            - https://github.com/xinglin/tpch
            - https://github.com/Chotom/tpch-mysql-implementation

            - https://github.com/spetrunia/tpcds-run-tool


            - https://github.com/stanislawbartkowski/mytpcds

            - https://github.com/FdeFabricio/tpcds-mysql

            Use this project:
                - https://github.com/QasimKhan5x/TPC-DS-MySQL

            - Just use TPCH as workload

05/06/2024 @ 5:19 PM (experiment 56)
    Database: tpcc_30
    Both index and knob tuning

    300 iterations

    Index tuner:
        {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003, 'tpcc_time':4, 'marl_step':marl_step}

    Knob tuner:
        {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4, 'marl_step':marl_step}

    Index reward: compute_team_reward_min

    Knob reward: compute_delta_avg_reward

    Output:
        outputs/marl_tuning_data/marl13.json

    Experience replay:
        experience_replay/dqn_index_tune/experience_replay_tpcc_30_2024-05-0617:56:55410465.json

    Testing to see if latency and throughput improve by forcing index selection to account more for the reward of the knob tuner

    Todo: use compute_delta_avg_reward for index reward


05/06/2024 @ 8:34 PM (experiment 57)
    Database: tpcc_30
    Both index and knob tuning

    300 iterations

    Index tuner:
        {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003, 'tpcc_time':4, 'marl_step':marl_step}

    Knob tuner:
        {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4, 'marl_step':marl_step}

    Index reward: compute_delta_avg_reward

    Knob reward: compute_delta_avg_reward

    Outputs:
        outputs/marl_tuning_data/marl14.json

    Experience replay:
        experience_replay/dqn_index_tune/experience_replay_tpcc_30_2024-05-0621:00:06438973.json

    No meaningful change from prior experiments

    Now trying the same reward function for both agents

    Ideas:
        - try compute_cost_delta_per_query_unscaled as reward function for both
            - turn off latency and throughput calculations
            - write insert, update, and deletion queries to add to workload as a second step

        - Rerun experiment 56, this time tuning knobs directly after index selection

            - 200 or 300 iterations for index, then 300 for knob

            - Run agent until convergence, then switch

        - Try with TPCH, using standard metrics as reward function

        - Just focus on optimizing latency or throughput, but not both
            - i.e use one or the other as the reward signal


05/07/2024 @ 4:25 PM (experiment 58)
    Database: tpcc_30
    Both index and knob tuning

    300 iterations

    Index tuner:
        {'weight_copy_interval':10, 'epsilon':1, 'epsilon_decay':0.003, 'tpcc_time':4, 'marl_step':marl_step}

    Knob tuner:
        {'replay_size':50, 'noise_scale':1.5, 'noise_decay':0.006, 'batch_size':40, 'tpcc_time':4, 'marl_step':marl_step}

    Index reward: compute_cost_delta_per_query_unscaled

    Knob reward: compute_cost_delta_per_query_unscaled

    Outputs (interleaved MARL):
        outputs/marl_tuning_data/marl15.json (possibly ignore)
        outputs/marl_tuning_data/marl16.json (good)
        outputs/marl_tuning_data/marl17.json (great results)
        outputs/marl_tuning_data/marl18.json (also great results)
        outputs/marl_tuning_data/marl19.json (ignore)
        outputs/marl_tuning_data/marl20.json (possibly ignore)
        outputs/marl_tuning_data/marl21.json (good)
        outputs/marl_tuning_data/marl22.json (good)
        outputs/marl_tuning_data/marl23.json (possibly ignore)

        Experience replay:
            experience_replay/dqn_index_tune/experience_replay_tpcc_30_2024-05-0717:29:57645925.json

    Outputs (interleaved, no MARL):
        outputs/marl_tuning_data/marl24.json
        outputs/marl_tuning_data/marl25.json
        outputs/marl_tuning_data/marl26.json
        outputs/marl_tuning_data/marl27.json
        outputs/marl_tuning_data/marl28.json
        outputs/marl_tuning_data/marl29.json
        outputs/marl_tuning_data/marl30.json

        Experience replay:
            experience_replay/dqn_index_tune/experience_replay_tpcc_30_2024-05-0911:32:20994976.json

    Outputs (sequential MARL)
        outputs/marl_tuning_data/marl32.json

        Experience replay:
            experience_replay/dqn_index_tune/experience_replay_tpcc_30_2024-05-0920:06:08138200.json

    Run over more epochs?

    Interleaving becomes effectively redundant when measuring the same thing (i.e having the same reward function)
    Also, any overcompensations can take place in subsequent epochs/the next agent switch


    Observations:
        - steady and constant decrease in workload cost while clear evidence of reward optimization/learning

    - Try TPCH, remove queries that take up exorbitant amount of time
        - use query planner!!

    - Check if sysbench has faster benchmarks for both read and write
        - Use sysbench OTLP if faster than TPCH
        - https://severalnines.com/blog/how-benchmark-performance-mysql-mariadb-using-sysbench/

    - adequately scale workload cost and then compare with latency and throughput

    - current experiments needed:
        - get more training runs for MARL
    
        - five training runs for interleaving, no state sharing

        - five training runs for staggered execution (agent after agent), no state sharing


05/08/2024 @ 6:41 PM (experiment 59)
    Database: tpcc_30
    ONLY Index tuning

    testing compute_cost_delta_per_query_unscaled_geometric

    250 iterations

    Outputs:
        outputs/tuning_data/rl_dqn21.json

    Oberservations: does not work. Not a strong reward signal

06/11/2024 @ 4:05 PM (Experiment 60)
    Database: tpch1

    TPC-H queries chosen: [1, 2, 3, 4, 5, 6, 7]

    Testing refractored index tuner

    Notes:
        - not enough of a problem gradient
        - might want to drop a few indexes
        - might want to try on all the queries (or more of them)
        - try on a larger database


        - for latency and throughput, just compute min(latency, throughput), do not find difference between begining and end
    
06/13/2024 @2:00 PM (Experiment 61)
    Database: sysbench_tune

    - testing refractored knob tuner

    - todo: try using reward function that only accounts for throughput

06/13/2024 @ 6:39 PM (Experiment 62)

    Database: sysbench_tune

    - Reward function: compute_sysbench_reward_throughput

    - Todo: try with is_marl = True

    outputs/knob_tuning_data/rl_ddpg15.json

    - potential reward functions:
        - max instead of min (to capture throughput)

        - geometric mean with weighted throughput?

        - normalize and then find min

        - (w*((latency_old - latency_new)/latency_old) + (throughput_new - throughput_old)/throughput_new)/2
                - w = 0.01

            could also simply add: 0.01*l1 + th


    - use sklearn.preprocessing.normalize?


06/14/2024 @ 10:50 AM (Experiment 63)
    Database: sysbench_tune

    Reward function: compute_sysbench_weighted_avg_reward

    Observations:
        - currently a tradeoff between throughput and latency
        - latency weight is directly proportional to its priority in tuning

    Outputs:
        outputs/knob_tuning_data/rl_ddpg16.json
        outputs/knob_tuning_data/rl_ddpg17.json

    Todo:
        - Run on TPC-C


06/14/2024 @ 2:40 PM (Experiment 64)
    Database: sysbench_tune

    Reward function: compute_sysbench_discounted_latency_reward


    outputs/knob_tuning_data/rl_ddpg18.json


06/27/2024 @ 3:06 PM (Experiment 65)
    Database: sysbench_tune

    Reward function: compute_sysbench_discounted_latency_reward

    Iterations: 800

    Investigating model convergence over a larger number of iterations

    Outputs:
        outputs/knob_tuning_data/rl_ddpg19.json

    Ideas to achieve convergence:
        - train over more epochs, not just iterations
        - try a different (simpler?) model architecture
        - try a new reward function that uses throughput as signal
        - play around with random noise and annealing
        - play around with the size of tau
        - increase size of buffer sample to feed to model


07/05/2024 @ 2:05 PM (Experiment 66)

    Database: sysbench_tune

    Iterations: 600

    Reward function: compute_sysbench_reward_throughput

    Noise decay: 0.05

    Outputs:
        outputs/knob_tuning_data/rl_ddpg20.json

    No improvement

    Ideas:
        - stronger penalty for decrease in changes
        - could store a "best seen delta" and reward or penalize based on how delta(t2, t1) compares



07/05/2024 @ 7:41 PM (Experiment 67)

    Database: sysbench_tune

    Iterations: 400

    Batch size: 50, slightly larger than normal (increase it further?)

    Reward function: compute_sysbench_reward_throughput_max_adjust

    Outputs:
        outputs/knob_tuning_data/rl_ddpg21.json

    Promising, need more iterations

07/05/2024 @ 8:07 PM (Experiment 68)

    Database: sysbench_tune

    Iterations: 800

    Batch size: 50

    Reward function: compute_sysbench_reward_throughput_max_adjust

    Outputs:
        outputs/knob_tuning_data/rl_ddpg22.json

    No change. Still volatile


    When massive dropoff in reward/throughput occurs, a few things could be done:
        1. Reapply the knob configuration at the previous maximum

        2. Use a lookahead. At each iteration, check if the net throughput increases or decreases
            if it decreases below a certain threshold (perhaps the default throughput). Then retain the current configuration
                - Try adding random noise to current config for five iterations, and if not improvement is found, continue
            
        3. Or, increase the annealing scale to encourage exploration. 


07/06/2024 @ 9:41 AM (Experiment 69)

    Database: sysbench_tune

    Iterations: 800

    Batch size: 50

    Reward function: compute_sysbench_reward_throughput_raw

    Trying a simpler network architecture

    Outputs:
        smoother = whittaker_smoother

    Todo:
        - retry with new architecture and non-annealing 
        - Switch to ReLU from tanh
        - train over more episodes

        https://ai.stackexchange.com/questions/22945/ddpg-doesnt-converge-for-mountaincarcontinuous-v0-gym-environment
        https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b

07/08/2024 @ 10:05 AM (Experiment 70)

    Database: sysbench_tune

    Iterations: 600

    Batch size: 50

    Reward function: compute_sysbench_reward_throughput_raw

    Trying simpler network architecture

    Outputs:
        outputs/knob_tuning_data/rl_ddpg24.json
        outputs/knob_tuning_data/rl_ddpg25.json

    Does the annealing coeficient become too small? (Done)

    Penalize if throughput falls below running average

    Decrease the learning rate (Done)

    Priority experience replay

    Lower tau value

    Re-read CDBTune paper

    Need consistent, stationary reward signal. Go back to rewarding basic points?

    https://openreview.net/pdf?id=64vORA8HUNv


    Switch to ReLU from Tanh


09/03/2024 @ 1:37 PM (Experiment 71)
    Database: sysbench_tune

    Iterations: 600

    Batch size: 50

    Reward function: compute_sysbench_reward_throughput_delta

    Outputs:
        outputs/knob_tuning_data/rl_ddpg26.json

    {
        'database': 'sysbench_tune',
        'episodes': 1,
        'replay_size': 60,
        'noise_scale': 1.5,
        'noise_decay': 0.05,
        'batch_size': 50,
        'min_noise_scale': 1*10**-4,
        'alr': 1*10**-5,
        'clr': 1*10**-5
        'workload_exec_time': 10,
        'marl_step': 50,
        'iterations': 600,
        'reward_func': 'compute_sysbench_reward_throughput_delta',
        'reward_signal': 'sysbench_latency_throughput',
        'is_marl': True
    }

    - clipped noise scale, smaller learning rates, and delta reward function

    Eliminate all noise after a certain number of iterations?

    Increase "workload_exec_time"

    Clip rewards? That way they are all within a particular range

    Catastrophic forgetting?
        - Possible environment issue

    Remove normalization of rewards in main body of code?


09/03/2024 @ 4:34 PM (Experiment 72)
    Database: sysbench_tune

    Iterations: 600

    Batch size: 50

    {
        'database': 'sysbench_tune',
        'episodes': 1,
        'replay_size': 40,
        'noise_scale': 1.5,
        'noise_decay': 0.05,
        'batch_size': 50,
        'min_noise_scale': None,
        'alr': 1*10**-4,
        'clr': 1*10**-4,
        'workload_exec_time': 10,
        'marl_step': 50,
        'iterations': 600,
        'updates': 20,
        'tau': 0.9,
        'reward_func': 'compute_sysbench_reward_throughput',
        'reward_signal': 'sysbench_latency_throughput',
        'is_marl': True
    }

    Use a larger random sample size?

    Run multiple update rounds per iteration?

    Perhaps it needs to be trained for many hours?

    Outputs:
        outputs/knob_tuning_data/rl_ddpg27.json


    Increase random batch size?

    Replace knob generation with scheme from here: https://openreview.net/pdf?id=64vORA8HUNv

    Reduce the number of knobs being tuned

    Next steps:
        - reduce number of knobs
        - decrease learning rate further
        - reduce noise to 0 after certain period of time

    Possible reward function: 
        reward more if better than average
        else penalize

09/04/2024 @ 10:46 AM (Experiment 73)
    Database: sysbench_tune

    atlas_knob_tune({
        'database': 'sysbench_tune',
        'episodes': 1,
        'replay_size': 60,
        'noise_scale': 1.5,
        'noise_decay': 0.05,
        'batch_size': 50,
        'min_noise_scale': None,
        'alr': 1*10**-4,
        'clr': 1*10**-4,
        'workload_exec_time': 10,
        'marl_step': 50,
        'iterations': 600,
        'updates': 20,
        'tau': 0.9,
        'reward_func': 'compute_sysbench_reward_throughput',
        'reward_signal': 'sysbench_latency_throughput',
        'is_marl': True
    })

    Trying on fewer knobs (only six)

    To do next:
        - decrease learning rate further
        - try on larger random batch sample size
        - reduce noise to 0 after certain period of time
        - instead of actor_loss = pqv.mean(), try:
                actor_loss = pqv.sum()

        - use epsilon greedy to either return the raw action from the model or add noise to it

        - learning rate annealing

        - may just need to train for longer on larger table sizes


        - after x number of iterations (or after noise scale decreases to a specific amount),
            do not add new action to memory buffer. This way, it will not become overrepresented with the same values.

            noise_scale cutoff: 5.534631512972999e-05

        - need to significantly decrease noise scale at the start?

        - perhaps do not anneal noise factor?

    Outputs:
        outputs/knob_tuning_data/rl_ddpg28.json


09/04/2024 @ 10:46 AM (Experiment 74)
    Database: sysbench_tune

    atlas_knob_tune({
        'database': 'sysbench_tune',
        'episodes': 1,
        'replay_size': 60,
        'noise_scale': 0.5,
        'noise_decay': 0.05,
        'batch_size': 50,
        'min_noise_scale': None,
        'alr': 1*10**-4,
        'clr': 1*10**-4,
        'workload_exec_time': 10,
        'marl_step': 50,
        'iterations': 600,
        'updates': 20,
        'tau': 0.9,
        'reward_func': 'compute_sysbench_reward_throughput',
        'reward_signal': 'sysbench_latency_throughput',
        'is_marl': True
    })

    Making it more likely to explore for longer

    Ideas:
        after every 100 iterations, clean up sysbench test suite

    Should it simply terminate after best solution found?

    Outputs:
        outputs/knob_tuning_data/rl_ddpg29.json

09/04/2024 @ 6:02 PM (Experiment 75)
    Database: sysbench_tune

    atlas_knob_tune({
        'database': 'sysbench_tune',
        'episodes': 1,
        'replay_size': 60,
        'noise_scale': 0.5,
        'noise_decay': 0.008,
        'batch_size': 50,
        'min_noise_scale': None,
        'alr': 1*10**-4,
        'clr': 1*10**-4,
        'workload_exec_time': 10,
        'marl_step': 50,
        'iterations': 600,
        'updates': 10,
        'tau': 0.9,
        'reward_func': 'compute_sysbench_reward_throughput_scaled',
        'reward_signal': 'sysbench_latency_throughput',
        'is_marl': True
    })

    Decrease Tau further?
    Allow for more exploration?

    To do:
        - reset benchmark every "n" iterations
        - only sample within "m" iterations
        - perhaps just detect early convergence and terminate


    Outputs:
        outputs/knob_tuning_data/rl_ddpg30.json


09/05/2024 @ 12:30 PM (Experiment 76)

    atlas_knob_tune({
        'database': 'sysbench_tune',
        'episodes': 1,
        'replay_size': 60,
        'noise_scale': 0.5,
        'noise_decay': 0.008,
        'batch_size': 50,
        'min_noise_scale': None,
        'alr': 1*10**-4,
        'clr': 1*10**-4,
        'workload_exec_time': 10,
        'marl_step': 50,
        'iterations': 600,
        'updates': 10,
        'tau': 0.9,
        'reward_func': 'compute_sysbench_reward_throughput_scaled',
        'reward_signal': 'sysbench_latency_throughput',
        'env_reset': {
            'steps': 50,
            'func': 'sysbench_env_reset'
        },
        'is_marl': True
    })

    Outputs:
        outputs/knob_tuning_data/rl_ddpg31.json

    To do:
        - Investigate output buffer
        - anneal the learning rate
        - larger random batch sample size
        - ignore buffer samples after x iterations
        - just detect early convergence
        - torch.Adam weight_decay
        - look into momentum in SGD
        - decrease noise scale
        - https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR
        - https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling


09/06/2024 @ 1:51 AM (Experiment 77)

    atlas_knob_tune({
        'database': 'sysbench_tune',
        'episodes': 1,
        'replay_size': 60,
        'noise_scale': 0.5,
        'noise_decay': 0.015,
        'batch_size': 200,
        'min_noise_scale': None,
        'alr': 1*10**-4,
        'clr': 1*10**-4,
        'workload_exec_time': 10,
        'marl_step': 50,
        'iterations': 400,
        'updates': 10,
        'tau': 0.9,
        'reward_func': 'compute_sysbench_reward_throughput_scaled',
        'reward_signal': 'sysbench_latency_throughput',
        'env_reset': None,
        'is_marl': True
    })

    Outputs:
        outputs/knob_tuning_data/rl_ddpg32.json

    larger batch size

    To do:
        - increase tau
        - remove weight_decay

    Look at results from:
        - experiment 70
        - experiment 70_1
        - experiment 72
        - experiment 77